# 认识CA2迁移对抗攻击框架
## 背景
`插入PPT-2`

##### 现实场景（Real World）下，由于诸多限制的存在，如查询次数限制、收费限制，导致经典的基于查询的黑盒攻击方式并不适用。另外，由于对抗样本本身的迁移特点（也即泛化特性）。基于以上两点，诞生了一类被称为“0接触”攻击的另类黑盒攻击模式--迁移攻击。
##### 迁移攻击模式下，攻击算法攻击本地模型构造对抗样本，然后使用本地构造的对抗样本直接尝试攻击现实场景的目标模型。
##### 上述模型能够有效的关键理论假设是：针对同类数据集不同的网络模型经过训练会学习到相似的特征。当然，这也是为什么对抗样本能够具备泛化性的理论基础之一。
##### 由于诸如对抗训练的防御机制被广泛应用，经典的黑盒算法攻击成功率降低，并且依赖大量的查询（即大量迭代），且生成的对抗样本泛化性较弱（过拟合）；而白盒攻击，如基于梯度/基于特征的白盒攻击算法，生成的对抗样本具备相对较优的泛化性，且迭代数量明显优于黑盒算法。因此，多数迁移攻击算法均选择在本地使用白盒攻击来构造对样样本。

## 迁移对抗攻击的数学抽象
`插入PPT-3`

##### 如文章[CA2]中描述，迁移攻击即是：通过攻击模型y_w寻找对抗样本x_adv，期望x_adv对目标模型y_b也同样有效。

## 现有挑战1：迁移性饱和陷阱
`插入PPT-4`

##### 对对抗样本经典白盒攻击算法（如FGSM）有一定了解的话，可知，构造对抗样本的过程，可以抽象为对损失函数的一种反向优化（这里反向指区别于模型训练过程的优化）。
##### 文章[CA2]的分析实验表明，几个经典的学界主流的迁移算法（内集成了白盒算法），随着迭代次数的提升，并不能有效增强迁移效果，文中称为“迁移性饱和陷阱”，也即是复杂优化问题中常见的“局部最优”问题。

## 现有挑战2：过拟合导致的算力依赖
`插入PPT-5`

##### 学界很多研究者的分析实验均表明，存在如下问题：在白盒预训练模型上表现优异，但泛化至黑盒防御模型时，效果大幅度降低，也即另一个机器学习领域常见的问题--过拟合。
##### 那么如何能在本地尽量学到接近未知目标模型的决策边界信息呢？[MIM]中给出的缓解思路是在本地训练多个异构的模型，用于模拟近似目标模型的决策边界。但这会导致攻击者需要花费大量计算资源在本地训练多个模型，存在较大的局限性（算力依赖）。

## 挑战1缓解思路：知识蒸馏，循环优化
`插入PPT-6`

##### 针对局部最优问题，[CA2]给出的缓解方案是“循环优化（RO）”。
##### 已知[MIM]既是出于提升优化效能对经典算法FGSM的改进，引入的是动量优化策略（在模型训练中也被大量采用）。
##### 而循环优化策略，则是进一步对[MIM]进行了优化，其核心思想是，在优化搜索的迭代过程中，使用“积累经验（即文中的知识蒸馏），多次出发”的策略，替代[MIM]中“一条路走到黑”的策略。直观体现可见图4-3，将1条搜索路径（12次迭代），分为3次走完，每次均从起点重新出发，但保留上一次的部分经验。具体差异可以对比公式（4-3/4-4）与（4-5/4-6/4-7）。
##### 参考上述PPT页右侧伪代码，帮助理解RO与原[MIM]的不同之处。

## 挑战2缓解思路-1：虚拟模型，自集成
`插入PPT-7`
`插入PPT-8`

##### 前面提到针对过拟合问题，[MIM]提出了攻击多个本地模型的集成策略，但面临算力依赖的局限性。
##### 对此，[CA2]提出了“虚拟模型集成（VME）”策略，通过添加额外的网络层，在无需重复训练模型的情况下，得到多个拥有不同决策边界的新模型，参考上述PPT中图4-4示意图。文中添加高斯卷积层达到其目的。
##### 由于在模型入口处添加高斯卷积层，在数学上，等价于对输入进行高斯卷积计算后，再输入到原模型。进一步，在损失函数层面的表示，则是先进行高斯卷积计算，再计算损失函数，见上述PPT中公式（4-9）。可参考PPT中右下方伪代码帮助理解。


## 挑战2缓解思路-2：数据增强
`插入PPT-9`

##### 针对过拟合问题，[CA2]借鉴模型训练领域使用“数据增强方法”提升收敛速度，缓解过拟合问题的思路，提出了“偏移增强 （D2A）”策略。
##### 如示意图4-6所展示的，采用偏移增强后的样本得到的梯度方向，要优于经典梯度下降方法得到的梯度方向。可参考PPT右侧伪代码帮助理解。

## 挑战2缓解思路：数据增强 + 虚拟集成
`插入PPT-10`

##### 结合上述的“虚拟模型集成（VME）”策略和“偏移增强 （D2A）”策略就形成了[CA2]文中的“自增强（SA）”策略。公式（4-12）为组合VME和D2A策略后的综合损失函数。
##### 体现在实现中，可参考PPT中伪代码。

## CA2 = 循环优化 + (数据增强 + 虚拟集成)
`插入PPT-11`

##### 继续将前面提到的“循环优化 RO”策略与“自增强（SA）”策略结合（如图4-1），就形成了CA2框架策略。可参考右侧伪代码帮助理解RO与SA的集成机制。
##### 如PPT中图4-12所示，[CA2]文中实验表面CA2（以及升级版CA2-SIM+，后面会讲解），在迁移成功率上胜过了当时所有的主流算法，表现出较大的优化提升。

## 为什么CA2自称“框架”而非“算法”？
`插入PPT-12`

##### 因为[CA2]提出的RO和SA策略均表现出较为容易与其它基于梯度的白盒算法进行集成的特点，并且文中以集成了SIM、TIM、DIM 3种先进算法为实例（CA2-SIM+版本），并在实验中变现出十分亮眼的效果。证明RO & SA策略有效性的同时，也证明了其易于集成未来新算法的特点。

## 学界提升对抗样本迁移性能的2个主要方向 与 CA2迁移攻击框架
`插入PPT-13`

##### [SIM]文中提到，当前学界在提升对抗样本迁移性能这一研究方向，主要有两个思路：一个是通过提升优化算法效能，期望能够更好地搜索得到泛化性更强的样本，缓解的是局部最优问题；
##### 另一个是实施模型增强，期望获得更多样的决策边界信息，进而使样本的泛化性能提升，缓解的是过拟合问题。
##### 上述PPT将主流的一些研究成果按 提升优化算法 和 模型增强 分类。相比于提升优化算法效能，模型增强方向的成果明显更多。
##### 模型增强这一方向，可认为传承至[MIM]中集成模型的思路（多个模型强于单个模型），为了缓解训练多个异构模型的算力依赖局限，后续研究成果多体现在如何通过构建虚拟模型，来获得多个具备不同决策边界的新模型，如SIM、VME。
##### 构建虚拟模型之所以有效的关键理论基础是[SIM]文中提到的：loss-preserving transformation，“变换不变性”，即存在某些变换S(x)，当输入模型的样本x经过变换S后再输入模型，对损失函数的影响有限，即J(x, y) ≈ J(S(x), y)。至于为什么引入变换S(x)等价于获得新的模型，可以回顾前文讲解“虚拟模型集成（VME）”策略部分。
##### 基于上述理论，VME采用的是高斯卷积变换、SIM采用的是像素缩放变换、D2A采用的是随机偏移变换、DIM采用的是随机概率性缩放&填充变换、TIM采用的是大量平移变换，均可认为是模型增强的范畴。
##### 用数学表示，虽然理念上是模型增强：y1(x) -> y2(x) ，其实落地到具体实现，都是对样本x的变换：y(x) -> y(S(x))，等价于模型增强。
##### 提升优化算法性能方向比较好理解，但应该比较难。目前较多应用的是动量优化和牛顿动量优化（NAG），这两者都是对经典梯度下降算法的优化，有兴趣可以参考知乎文章[[IMG-NAG]](https://zhuanlan.zhihu.com/p/60088231)。如果将NAG看做是在搜索时对方向决策的改进，那么RO就是对整体搜索策略的改进：积累经验多次出发，因此RO可以轻易与MIM和NIM相结合。
##### 上述PPT右侧部分，也展示了CA2易于集成其它模型增强策略的特点。进而可以轻易地得到新的PLUS版本。


### 参考文献

###### [CA2]: 博士学位论文, 《面向计算机视觉的深度学习对抗攻防方法研究》第四章: 基于自增强方法的循环迁移攻击框架; 黄立峰.

###### [MIM]: Dong Y, Liao F, Pang T, et al. Boosting adversarial attacks with momentum[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 9185-9193.

###### [SIM]: Lin J, Song C, He K, et al. Nesterov accelerated gradient and scale invariance for adversarial attacks[C]//International Conference on Learning Representations. 2019: 1-11.

###### [NI-SI-FGSM Note]: MichaeI-Way, 《[论文笔记]NI-SI-FGSM》, https://zhuanlan.zhihu.com/p/497026313.

###### [IMG-NAG]: 郑思座, 《谈谈优化算法之一（动量法、Nesterov法、自然梯度法）》, https://zhuanlan.zhihu.com/p/60088231.

